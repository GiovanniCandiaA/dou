{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FXDnBXHIfQTm"
   },
   "outputs": [],
   "source": [
    "# CNN_8_7\n",
    "# Author: Matthew Dixon\n",
    "# Version: 1.0 (24.7.2019)\n",
    "# License: MIT\n",
    "# Email: matthew.dixon@iit.edu\n",
    "# Notes: tested on Mac OS X with Python 3.6\n",
    "# Citation: Please cite the following reference if this notebook is used for research purposes:\n",
    "# Dixon M.F., I. Halperin and P. Bilokon, Machine Learning in Finance: From Theory to Practice, Springer Graduate textbook Series, 2020. \n",
    "# Notebook adapted from solution provided by Noah Grudowski. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6AWn-1-ogJLo"
   },
   "source": [
    "### Load Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FrlQQLdqgNNz"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t7ek9mx-gPGm"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4X7jIOs5gQjw"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM, GRU, SimpleRNN\n",
    "from keras import optimizers\n",
    "from keras.models import load_model\n",
    "from keras.layers import Convolution1D, Dense, MaxPooling1D, Flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d5GPT0UygSBQ"
   },
   "source": [
    "### Load HFT Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 390
    },
    "colab_type": "code",
    "id": "jfD1EZT2gbKO",
    "outputId": "941756a7-2fb0-4a3a-dcf5-7ec0012f7910"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../data/HFT.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y7Bc1iYagddm"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.515301</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.710953</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.515301</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.710953</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.515301</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.710953</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.515301</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.710953</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.515301</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.710953</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_1  feature_2  feature_3  label\n",
       "0   0.515301       0.72   0.710953      0\n",
       "1   0.515301       0.72   0.710953      0\n",
       "2   0.515301       0.72   0.710953      0\n",
       "3   0.515301       0.72   0.710953      0\n",
       "4   0.515301       0.72   0.710953      0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eQKSMWsjgeQU"
   },
   "source": [
    "We consider a univariate prediction problem where the time series is given by 'feature_3' in the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7ZajaY2Iggan"
   },
   "outputs": [],
   "source": [
    "use_features = 'feature_3' # continuous input\n",
    "target = 'feature_3' # continuous output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EZMumn_mgiep"
   },
   "source": [
    "## Part a: Stationarity with Augmented Dickey-Fuller Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aLeaf687gnYu"
   },
   "source": [
    "The null hypothesis of the Augmented Dickey-Fuller is that there is a unit root, with the alternative that there is no unit root. If the p-value is above $(1-\\alpha)$, then we cannot reject that there is a unit root. Note that a subset of the time series is used to reduce the computation time of the test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vH0DLTHsgqeO"
   },
   "outputs": [],
   "source": [
    "adf, p, usedlag, nobs, cvs,aic=sm.tsa.stattools.adfuller(df[use_features][:200000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mmy2Sl_gguBw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.970572659864007 0.0015726872209325432 80 199919 {'1%': -3.430382710167448, '5%': -2.8615544574611698, '10%': -2.566777695186804} -1679778.349426464\n"
     ]
    }
   ],
   "source": [
    "print(adf, p, usedlag, nobs, cvs,aic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the above output, we have a test statistic of -3.97057.  Thus, we reject the null hypothesis, as there is reasonable evidence to conclude that this dataset is stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NnU111OHgu5y"
   },
   "source": [
    "## Part b: Estimate Partial Auto-Correlation Function and Determine Optimum Lag (99% Confidence)\n",
    "The estimated partial auto-correlation function (PACF) can be used to identify the order of an autoregressive time series model. Values of $|\\tau_h|$ greater or equal to $\\frac{\\Phi^{-1}(\\alpha)}{\\sqrt{T}}$, where T is the number of observations and $\\Phi(z)$ is the standard normal CDF, are significant lag $h$ partial autocorelations at the $\\alpha$ confidence level.\n",
    "\n",
    "We use the stattools package to estimat the PACF. The 'nlags' parameter is the maximum number of lags used for PACF estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mK1cIjKtgznS"
   },
   "outputs": [],
   "source": [
    "pacf=sm.tsa.stattools.pacf(df[use_features], nlags=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BhAaBvMSg3QY"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.99978073, 0.00645883, 0.02457632, 0.02450646,\n",
       "       0.01566672, 0.01228706, 0.01406852, 0.00552327, 0.01106975,\n",
       "       0.0097814 , 0.00528616, 0.00271307, 0.00482261, 0.00514608,\n",
       "       0.01184934, 0.0107241 , 0.00366119, 0.00550863, 0.00744828,\n",
       "       0.00379451, 0.00335972, 0.00854415, 0.00654713, 0.00155662,\n",
       "       0.00443114, 0.00538996, 0.00230699, 0.00812299, 0.00166796,\n",
       "       0.00477462])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pacf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T3oydg2kg3yp"
   },
   "outputs": [],
   "source": [
    "n_steps=np.where(np.array(np.abs(pacf)>2.58/np.sqrt(len(df[use_features])))==False)[0][0] -1\n",
    "#Note that a z-score of 2.58 corresponds to the 99% confidence interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Za_LTbtmg6pi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n"
     ]
    }
   ],
   "source": [
    "print(n_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we have shown that the optimum lag at the 99% confidence level is 23. Note that this value for the number of lags is a little high, and may be more than necessary. Thus, this could unnaturally lead to a high order model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data for Parts c, d, and e\n",
    "### Splitting and Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "icygZm4ig8MT"
   },
   "source": [
    "This may lead to a high order model, with more lags than strictly necessary. We could view this value, informally, as an upper bound on the number of lags needed. We can also simply identify the order of the model based on the plot of the PACF. In this case, a minimum of 2 lags appears satisfactory, although more may be needed. Unlike autoregressive models, the advantage of using fewer parameters is purely computational as adding more lags does not increase the number of parameters, only the size of the tensorial representation of the sequence data in TensorFlow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xX90PqaQg-sr"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x10b60860>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(pacf, label='pacf')\n",
    "plt.plot([2.58/np.sqrt(len(df[use_features]))]*30, label='99% confidence interval')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7HWRgQBDhCUB"
   },
   "source": [
    "### Splitting\n",
    "Split the training and test set by using the first 80% of the time series and the remaining 20% for the test set. Note that the test set must be in the future of the training set to avoid look-ahead bias. Also, random sampling of the data can not be used as this would eliminate the auto-correlation structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TFjU1zQwhD1B"
   },
   "outputs": [],
   "source": [
    "train_weight = 0.8\n",
    "split = int(len(df)*train_weight)\n",
    "df_train = df.iloc[:split]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZL2rIpMHhGbs"
   },
   "source": [
    "### Scaling\n",
    "Standardization of the data is important to avoid potential scaling difficulties in the fitting of the model. When there is more than one feature (covariate), scaling avoids one feature dominating over another due to disparate scales.\n",
    "\n",
    "To avoid introducing a look-ahead bias into the prediction, we must re-scale the training data without knowledge of the test set. Hence, we will simply standardize the training set using the mean and standard deviation of the training set and not the whole time series. Additionally, to avoid introducing a systematic bias into test set, we use the identical normalization for the test set - the mean and standard deviation of the training set are used to normalize the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IweYwslshIll"
   },
   "outputs": [],
   "source": [
    "mu = np.float(df_train[use_features].mean())\n",
    "sigma = np.float(df_train[use_features].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4lMxzrWGhLV4"
   },
   "outputs": [],
   "source": [
    "std_df_train = df_train[use_features].apply(lambda x: (x - mu) / sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8f71Lo99hM9g"
   },
   "outputs": [],
   "source": [
    "df_test = df.iloc[split:]\n",
    "std_df_test = df[use_features].apply(lambda x: (x - mu) / sigma).iloc[split:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part c: Evaluating MSE in-sample and out-of-sample Using 4 Filters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the number of filters is increased, the difference between the out-of-sample and in-sample mean-\n",
    "squared errors gets noticeably larger. While it can be seen that the in-sample error is minimized, the\n",
    "out-of-sample error begins to get larger and larger the more the number of filters is increased. This can\n",
    "be explained by the Bias-Variance tradeoff, as the increased number of filters is causing the model to\n",
    "become overfitted. As a result, this increased complexity in the model does minimize the in-sample\n",
    "error, but in consequence/tradeoff, the out-of-sample error is sacrificed. Put another way, we have a\n",
    "lower bias but a higher variance in this scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_timeseries_regressor(window_size, filter_length, nb_input_series=1, nb_outputs=1, nb_filter=4):\n",
    "    \n",
    "    model = Sequential((\n",
    "        # The first conv layer learns `nb_filter` filters (aka kernels), each of size ``(filter_length, nb_input_series)``.\n",
    "        # Its output will have shape (None, window_size - filter_length + 1, nb_filter), i.e., for each position in\n",
    "        # the input timeseries, the activation of each filter at that position.\n",
    "        Convolution1D(nb_filter=nb_filter, filter_length=filter_length, activation='relu', input_shape=(window_size, nb_input_series)),\n",
    "        Flatten(),\n",
    "        Dense(nb_outputs, activation='linear'),     # For binary classification, change the activation to 'sigmoid'\n",
    "    ))\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=['mae','mse'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_timeseries_instances(timeseries, window_size):\n",
    "    timeseries = np.asarray(timeseries)\n",
    "    assert 0 < window_size < timeseries.shape[0]\n",
    "    X = np.atleast_3d(np.array([timeseries[start:start + window_size] for start in range(0, timeseries.shape[0] - window_size)]))\n",
    "    y = timeseries[window_size:]\n",
    "    q = np.atleast_3d([timeseries[-window_size:]])\n",
    "    return X, y, q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_timeseries(timeseries, window_size):\n",
    "\n",
    "    filter_length = 5\n",
    "    nb_filter = 100\n",
    "    timeseries = np.atleast_2d(timeseries)\n",
    "    if timeseries.shape[0] == 1:\n",
    "        timeseries = timeseries.T       # Convert 1D vectors to 2D column vectors\n",
    "\n",
    "    nb_samples, nb_series = timeseries.shape\n",
    "    model = make_timeseries_regressor(window_size=window_size, filter_length=filter_length, nb_input_series=nb_series, nb_outputs=nb_series, nb_filter=nb_filter)\n",
    "    model.summary()\n",
    "\n",
    "    X, y, q = make_timeseries_instances(timeseries, window_size)\n",
    "    test_size = int(0.01 * nb_samples)           # In real life you'd want to use 0.2 - 0.5\n",
    "    X_train, X_test, y_train, y_test = X[:-test_size], X[-test_size:], y[:-test_size], y[-test_size:]\n",
    "    model.fit(X_train, y_train, nb_epoch=25, batch_size=2, validation_data=(X_test, y_test))\n",
    "\n",
    "    pred_out_sample = model.predict(X_test)\n",
    "    pred_in_sample = model.predict(X_train)\n",
    "    \n",
    "    print('Out of Sample Mean Squared Error: ',mean_squared_error(pred_out_sample,y_test))\n",
    "    print('In Sample Mean Squared Error: ',mean_squared_error(pred_in_sample,y_train))\n",
    "    \n",
    "    return model, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Noah PC\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", input_shape=(50, 1), filters=100, kernel_size=5)`\n",
      "  import sys\n",
      "C:\\Users\\Noah PC\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:19: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_3 (Conv1D)            (None, 46, 100)           600       \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 4600)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 4601      \n",
      "=================================================================\n",
      "Total params: 5,201\n",
      "Trainable params: 5,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 940 samples, validate on 10 samples\n",
      "Epoch 1/25\n",
      "940/940 [==============================] - 1s 1ms/step - loss: 0.0040 - mean_absolute_error: 0.0332 - mean_squared_error: 0.0040 - val_loss: 9.8232e-05 - val_mean_absolute_error: 0.0080 - val_mean_squared_error: 9.8232e-05\n",
      "Epoch 2/25\n",
      "940/940 [==============================] - 1s 876us/step - loss: 0.0016 - mean_absolute_error: 0.0184 - mean_squared_error: 0.0016 - val_loss: 6.5461e-05 - val_mean_absolute_error: 0.0060 - val_mean_squared_error: 6.5461e-05\n",
      "Epoch 3/25\n",
      "940/940 [==============================] - 1s 856us/step - loss: 0.0012 - mean_absolute_error: 0.0150 - mean_squared_error: 0.0012 - val_loss: 3.6515e-04 - val_mean_absolute_error: 0.0185 - val_mean_squared_error: 3.6515e-04\n",
      "Epoch 4/25\n",
      "940/940 [==============================] - 1s 866us/step - loss: 0.0013 - mean_absolute_error: 0.0154 - mean_squared_error: 0.0013 - val_loss: 2.4992e-05 - val_mean_absolute_error: 0.0044 - val_mean_squared_error: 2.4992e-05\n",
      "Epoch 5/25\n",
      "940/940 [==============================] - 1s 866us/step - loss: 0.0011 - mean_absolute_error: 0.0140 - mean_squared_error: 0.0011 - val_loss: 3.5412e-05 - val_mean_absolute_error: 0.0038 - val_mean_squared_error: 3.5412e-05\n",
      "Epoch 6/25\n",
      "940/940 [==============================] - 1s 856us/step - loss: 7.2923e-04 - mean_absolute_error: 0.0103 - mean_squared_error: 7.2923e-04 - val_loss: 5.5685e-05 - val_mean_absolute_error: 0.0063 - val_mean_squared_error: 5.5685e-05\n",
      "Epoch 7/25\n",
      "940/940 [==============================] - 1s 896us/step - loss: 9.4287e-04 - mean_absolute_error: 0.0115 - mean_squared_error: 9.4287e-04 - val_loss: 2.0549e-05 - val_mean_absolute_error: 0.0039 - val_mean_squared_error: 2.0549e-05\n",
      "Epoch 8/25\n",
      "940/940 [==============================] - 1s 869us/step - loss: 6.7756e-04 - mean_absolute_error: 0.0101 - mean_squared_error: 6.7756e-04 - val_loss: 2.5069e-05 - val_mean_absolute_error: 0.0045 - val_mean_squared_error: 2.5069e-05\n",
      "Epoch 9/25\n",
      "940/940 [==============================] - 1s 865us/step - loss: 6.5254e-04 - mean_absolute_error: 0.0107 - mean_squared_error: 6.5254e-04 - val_loss: 9.0801e-05 - val_mean_absolute_error: 0.0087 - val_mean_squared_error: 9.0801e-05\n",
      "Epoch 10/25\n",
      "940/940 [==============================] - 1s 870us/step - loss: 5.1723e-04 - mean_absolute_error: 0.0083 - mean_squared_error: 5.1723e-04 - val_loss: 3.0172e-04 - val_mean_absolute_error: 0.0168 - val_mean_squared_error: 3.0172e-04\n",
      "Epoch 11/25\n",
      "940/940 [==============================] - 1s 862us/step - loss: 6.8088e-04 - mean_absolute_error: 0.0114 - mean_squared_error: 6.8088e-04 - val_loss: 1.8734e-05 - val_mean_absolute_error: 0.0031 - val_mean_squared_error: 1.8734e-05\n",
      "Epoch 12/25\n",
      "940/940 [==============================] - 1s 867us/step - loss: 5.2954e-04 - mean_absolute_error: 0.0083 - mean_squared_error: 5.2954e-04 - val_loss: 1.4334e-05 - val_mean_absolute_error: 0.0026 - val_mean_squared_error: 1.4334e-05\n",
      "Epoch 13/25\n",
      "940/940 [==============================] - 1s 868us/step - loss: 6.4678e-04 - mean_absolute_error: 0.0106 - mean_squared_error: 6.4678e-04 - val_loss: 2.3409e-05 - val_mean_absolute_error: 0.0031 - val_mean_squared_error: 2.3409e-05\n",
      "Epoch 14/25\n",
      "940/940 [==============================] - 1s 886us/step - loss: 6.0637e-04 - mean_absolute_error: 0.0111 - mean_squared_error: 6.0637e-04 - val_loss: 3.4081e-05 - val_mean_absolute_error: 0.0045 - val_mean_squared_error: 3.4081e-05\n",
      "Epoch 15/25\n",
      "940/940 [==============================] - 1s 874us/step - loss: 4.8607e-04 - mean_absolute_error: 0.0088 - mean_squared_error: 4.8607e-04 - val_loss: 5.6270e-04 - val_mean_absolute_error: 0.0234 - val_mean_squared_error: 5.6270e-04\n",
      "Epoch 16/25\n",
      "940/940 [==============================] - 1s 877us/step - loss: 5.6316e-04 - mean_absolute_error: 0.0111 - mean_squared_error: 5.6316e-04 - val_loss: 4.7826e-05 - val_mean_absolute_error: 0.0059 - val_mean_squared_error: 4.7826e-05\n",
      "Epoch 17/25\n",
      "940/940 [==============================] - 1s 862us/step - loss: 5.2509e-04 - mean_absolute_error: 0.0108 - mean_squared_error: 5.2509e-04 - val_loss: 2.0569e-05 - val_mean_absolute_error: 0.0041 - val_mean_squared_error: 2.0569e-05\n",
      "Epoch 18/25\n",
      "940/940 [==============================] - 1s 876us/step - loss: 5.1177e-04 - mean_absolute_error: 0.0109 - mean_squared_error: 5.1177e-04 - val_loss: 3.2577e-05 - val_mean_absolute_error: 0.0052 - val_mean_squared_error: 3.2577e-05\n",
      "Epoch 19/25\n",
      "940/940 [==============================] - 1s 860us/step - loss: 5.7868e-04 - mean_absolute_error: 0.0111 - mean_squared_error: 5.7868e-04 - val_loss: 1.3598e-04 - val_mean_absolute_error: 0.0110 - val_mean_squared_error: 1.3598e-04\n",
      "Epoch 20/25\n",
      "940/940 [==============================] - 1s 917us/step - loss: 4.6119e-04 - mean_absolute_error: 0.0100 - mean_squared_error: 4.6119e-04 - val_loss: 1.7705e-05 - val_mean_absolute_error: 0.0037 - val_mean_squared_error: 1.7705e-05\n",
      "Epoch 21/25\n",
      "940/940 [==============================] - 1s 1ms/step - loss: 5.1645e-04 - mean_absolute_error: 0.0090 - mean_squared_error: 5.1645e-04 - val_loss: 0.0021 - val_mean_absolute_error: 0.0460 - val_mean_squared_error: 0.0021\n",
      "Epoch 22/25\n",
      "940/940 [==============================] - 1s 1ms/step - loss: 4.3051e-04 - mean_absolute_error: 0.0087 - mean_squared_error: 4.3051e-04 - val_loss: 2.4595e-05 - val_mean_absolute_error: 0.0033 - val_mean_squared_error: 2.4595e-05\n",
      "Epoch 23/25\n",
      "940/940 [==============================] - 1s 1ms/step - loss: 4.2130e-04 - mean_absolute_error: 0.0077 - mean_squared_error: 4.2130e-04 - val_loss: 2.7182e-04 - val_mean_absolute_error: 0.0159 - val_mean_squared_error: 2.7182e-04\n",
      "Epoch 24/25\n",
      "940/940 [==============================] - 1s 1ms/step - loss: 5.1063e-04 - mean_absolute_error: 0.0096 - mean_squared_error: 5.1063e-04 - val_loss: 5.9796e-05 - val_mean_absolute_error: 0.0067 - val_mean_squared_error: 5.9796e-05\n",
      "Epoch 25/25\n",
      "940/940 [==============================] - 1s 1ms/step - loss: 4.5765e-04 - mean_absolute_error: 0.0097 - mean_squared_error: 4.5765e-04 - val_loss: 5.0003e-05 - val_mean_absolute_error: 0.0054 - val_mean_squared_error: 5.0003e-05\n",
      "Out of Sample Mean Squared Error:  5.000300787430087e-05\n",
      "In Sample Mean Squared Error:  0.0003492821789039846\n"
     ]
    }
   ],
   "source": [
    "model, X_test, y_test = evaluate_timeseries(std_df_train[:1000], 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part d: Apply $L_1$ Regularization to Reduce the Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ð¿_1 regularization was applied using a ðœ† parameter that ranged from values between 0.01 and 1. As the\n",
    "value for ðœ† was increased (but the number of filters was kept constant), we were able to observe a\n",
    "smaller difference between the out-of-sample and in-sample mean-squared errors than we did in the\n",
    "previous case. This can also be explained by the Bias-Variance Tradeoff. In this scenario, we have a\n",
    "higher bias, but a smaller variance. By introducing ð¿_1 regularization to the model, we have essentially\n",
    "introduced a bias. As such, we observe higher values for the in-sample mean-squared errors than we\n",
    "did in the previous case. However, the differences between the in-sample and out-of-sample mean-\n",
    "squared errors does not increase as much as it did in the previous case, which exemplifies our smaller\n",
    "variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_timeseries_regressor(window_size, filter_length, nb_input_series=1, nb_outputs=1, nb_filter=4):\n",
    "\n",
    "    model = Sequential((\n",
    "        # The first conv layer learns `nb_filter` filters (aka kernels), each of size ``(filter_length, nb_input_series)``.\n",
    "        # Its output will have shape (None, window_size - filter_length + 1, nb_filter), i.e., for each position in\n",
    "        # the input timeseries, the activation of each filter at that position.\n",
    "        Convolution1D(kernel_regularizer=regularizers.l1(0.1), nb_filter=nb_filter, filter_length=filter_length, activation='relu', input_shape=(window_size, nb_input_series)),\n",
    "        #Added Kernel Regularizer Here: L1 regularization\n",
    "        Flatten(),\n",
    "        Dense(nb_outputs, activation='linear'),     # For binary classification, change the activation to 'sigmoid'\n",
    "    ))\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=['mae','mse'])\n",
    "   \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_timeseries_instances(timeseries, window_size):\n",
    "   \n",
    "    timeseries = np.asarray(timeseries)\n",
    "    assert 0 < window_size < timeseries.shape[0]\n",
    "    X = np.atleast_3d(np.array([timeseries[start:start + window_size] for start in range(0, timeseries.shape[0] - window_size)]))\n",
    "    y = timeseries[window_size:]\n",
    "    q = np.atleast_3d([timeseries[-window_size:]])\n",
    "    return X, y, q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_timeseries(timeseries, window_size):\n",
    "\n",
    "    filter_length = 5\n",
    "    nb_filter = 20\n",
    "    timeseries = np.atleast_2d(timeseries)\n",
    "    if timeseries.shape[0] == 1:\n",
    "        timeseries = timeseries.T       # Convert 1D vectors to 2D column vectors\n",
    "\n",
    "    nb_samples, nb_series = timeseries.shape\n",
    "    model = make_timeseries_regressor(window_size=window_size, filter_length=filter_length, nb_input_series=nb_series, nb_outputs=nb_series, nb_filter=nb_filter)\n",
    "    model.summary()\n",
    "\n",
    "    X, y, q = make_timeseries_instances(timeseries, window_size)\n",
    "    test_size = int(0.01 * nb_samples)           # In real life you'd want to use 0.2 - 0.5\n",
    "    X_train, X_test, y_train, y_test = X[:-test_size], X[-test_size:], y[:-test_size], y[-test_size:]\n",
    "    model.fit(X_train, y_train, nb_epoch=25, batch_size=2, validation_data=(X_test, y_test))\n",
    "\n",
    "    pred_out_sample = model.predict(X_test)\n",
    "    pred_in_sample = model.predict(X_train)\n",
    "    \n",
    "    print('Out of Sample Mean Squared Error: ',mean_squared_error(pred_out_sample,y_test))\n",
    "    print('In Sample Mean Squared Error: ',mean_squared_error(pred_in_sample,y_train))\n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Noah PC\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(kernel_regularizer=<keras.reg..., activation=\"relu\", input_shape=(50, 1), filters=20, kernel_size=5)`\n",
      "  import sys\n",
      "C:\\Users\\Noah PC\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:19: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_4 (Conv1D)            (None, 46, 20)            120       \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 920)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 921       \n",
      "=================================================================\n",
      "Total params: 1,041\n",
      "Trainable params: 1,041\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 4900 samples, validate on 50 samples\n",
      "Epoch 1/25\n",
      "4900/4900 [==============================] - 5s 960us/step - loss: 0.0516 - mean_absolute_error: 0.0414 - mean_squared_error: 0.0045 - val_loss: 0.0055 - val_mean_absolute_error: 0.0050 - val_mean_squared_error: 2.5724e-05\n",
      "Epoch 2/25\n",
      "4900/4900 [==============================] - 4s 824us/step - loss: 0.0086 - mean_absolute_error: 0.0319 - mean_squared_error: 0.0030 - val_loss: 0.0088 - val_mean_absolute_error: 0.0592 - val_mean_squared_error: 0.0035\n",
      "Epoch 3/25\n",
      "4900/4900 [==============================] - 4s 781us/step - loss: 0.0073 - mean_absolute_error: 0.0295 - mean_squared_error: 0.0027 - val_loss: 0.0042 - val_mean_absolute_error: 0.0099 - val_mean_squared_error: 9.9252e-05\n",
      "Epoch 4/25\n",
      "4900/4900 [==============================] - 4s 788us/step - loss: 0.0068 - mean_absolute_error: 0.0281 - mean_squared_error: 0.0025 - val_loss: 0.0048 - val_mean_absolute_error: 0.0221 - val_mean_squared_error: 4.8889e-04\n",
      "Epoch 5/25\n",
      "4900/4900 [==============================] - 4s 897us/step - loss: 0.0065 - mean_absolute_error: 0.0283 - mean_squared_error: 0.0024 - val_loss: 0.0040 - val_mean_absolute_error: 0.0057 - val_mean_squared_error: 3.2971e-05\n",
      "Epoch 6/25\n",
      "4900/4900 [==============================] - 5s 970us/step - loss: 0.0060 - mean_absolute_error: 0.0256 - mean_squared_error: 0.0022 - val_loss: 0.0043 - val_mean_absolute_error: 0.0243 - val_mean_squared_error: 5.9119e-04\n",
      "Epoch 7/25\n",
      "4900/4900 [==============================] - 5s 973us/step - loss: 0.0059 - mean_absolute_error: 0.0261 - mean_squared_error: 0.0021 - val_loss: 0.0039 - val_mean_absolute_error: 0.0080 - val_mean_squared_error: 6.4790e-05\n",
      "Epoch 8/25\n",
      "4900/4900 [==============================] - 5s 1ms/step - loss: 0.0056 - mean_absolute_error: 0.0242 - mean_squared_error: 0.0019 - val_loss: 0.0042 - val_mean_absolute_error: 0.0234 - val_mean_squared_error: 5.4620e-04\n",
      "Epoch 9/25\n",
      "4900/4900 [==============================] - 5s 1ms/step - loss: 0.0055 - mean_absolute_error: 0.0236 - mean_squared_error: 0.0018 - val_loss: 0.0040 - val_mean_absolute_error: 0.0101 - val_mean_squared_error: 1.0301e-04\n",
      "Epoch 10/25\n",
      "4900/4900 [==============================] - 5s 1ms/step - loss: 0.0056 - mean_absolute_error: 0.0257 - mean_squared_error: 0.0020 - val_loss: 0.0056 - val_mean_absolute_error: 0.0399 - val_mean_squared_error: 0.0016\n",
      "Epoch 11/25\n",
      "4900/4900 [==============================] - 5s 1ms/step - loss: 0.0054 - mean_absolute_error: 0.0227 - mean_squared_error: 0.0017 - val_loss: 0.0035 - val_mean_absolute_error: 0.0029 - val_mean_squared_error: 8.9734e-06\n",
      "Epoch 12/25\n",
      "4900/4900 [==============================] - 5s 1ms/step - loss: 0.0054 - mean_absolute_error: 0.0223 - mean_squared_error: 0.0018 - val_loss: 0.0039 - val_mean_absolute_error: 0.0170 - val_mean_squared_error: 2.8865e-04\n",
      "Epoch 13/25\n",
      "4900/4900 [==============================] - 5s 1ms/step - loss: 0.0054 - mean_absolute_error: 0.0230 - mean_squared_error: 0.0018 - val_loss: 0.0037 - val_mean_absolute_error: 0.0154 - val_mean_squared_error: 2.3773e-04\n",
      "Epoch 14/25\n",
      "4900/4900 [==============================] - 5s 1ms/step - loss: 0.0051 - mean_absolute_error: 0.0224 - mean_squared_error: 0.0016 - val_loss: 0.0038 - val_mean_absolute_error: 0.0131 - val_mean_squared_error: 1.7115e-04\n",
      "Epoch 15/25\n",
      "4900/4900 [==============================] - 5s 1ms/step - loss: 0.0052 - mean_absolute_error: 0.0213 - mean_squared_error: 0.0017 - val_loss: 0.0069 - val_mean_absolute_error: 0.0546 - val_mean_squared_error: 0.0030\n",
      "Epoch 16/25\n",
      "4900/4900 [==============================] - 5s 1ms/step - loss: 0.0051 - mean_absolute_error: 0.0218 - mean_squared_error: 0.0016 - val_loss: 0.0037 - val_mean_absolute_error: 0.0091 - val_mean_squared_error: 8.3310e-05\n",
      "Epoch 17/25\n",
      "4900/4900 [==============================] - 5s 1ms/step - loss: 0.0049 - mean_absolute_error: 0.0206 - mean_squared_error: 0.0014 - val_loss: 0.0035 - val_mean_absolute_error: 0.0062 - val_mean_squared_error: 3.8864e-05\n",
      "Epoch 18/25\n",
      "4900/4900 [==============================] - 5s 1ms/step - loss: 0.0051 - mean_absolute_error: 0.0215 - mean_squared_error: 0.0016 - val_loss: 0.0034 - val_mean_absolute_error: 0.0103 - val_mean_squared_error: 1.0650e-04\n",
      "Epoch 19/25\n",
      "4900/4900 [==============================] - 5s 1ms/step - loss: 0.0050 - mean_absolute_error: 0.0210 - mean_squared_error: 0.0016 - val_loss: 0.0045 - val_mean_absolute_error: 0.0042 - val_mean_squared_error: 1.8124e-05\n",
      "Epoch 20/25\n",
      "4900/4900 [==============================] - 5s 1ms/step - loss: 0.0050 - mean_absolute_error: 0.0218 - mean_squared_error: 0.0016 - val_loss: 0.0041 - val_mean_absolute_error: 0.0212 - val_mean_squared_error: 4.5176e-04\n",
      "Epoch 21/25\n",
      "4900/4900 [==============================] - 5s 1ms/step - loss: 0.0050 - mean_absolute_error: 0.0219 - mean_squared_error: 0.0016 - val_loss: 0.0046 - val_mean_absolute_error: 0.0053 - val_mean_squared_error: 2.8433e-05\n",
      "Epoch 22/25\n",
      "4900/4900 [==============================] - 5s 1ms/step - loss: 0.0050 - mean_absolute_error: 0.0221 - mean_squared_error: 0.0016 - val_loss: 0.0036 - val_mean_absolute_error: 0.0020 - val_mean_squared_error: 4.1323e-06\n",
      "Epoch 23/25\n",
      "4900/4900 [==============================] - 5s 1ms/step - loss: 0.0047 - mean_absolute_error: 0.0206 - mean_squared_error: 0.0014 - val_loss: 0.0042 - val_mean_absolute_error: 0.0308 - val_mean_squared_error: 9.4905e-04\n",
      "Epoch 24/25\n",
      "4900/4900 [==============================] - 5s 1ms/step - loss: 0.0047 - mean_absolute_error: 0.0203 - mean_squared_error: 0.0014 - val_loss: 0.0036 - val_mean_absolute_error: 0.0134 - val_mean_squared_error: 1.7891e-04\n",
      "Epoch 25/25\n",
      "4900/4900 [==============================] - 5s 1ms/step - loss: 0.0050 - mean_absolute_error: 0.0235 - mean_squared_error: 0.0017 - val_loss: 0.0037 - val_mean_absolute_error: 0.0226 - val_mean_squared_error: 5.1050e-04\n",
      "Out of Sample Mean Squared Error:  0.000510503708712955\n",
      "In Sample Mean Squared Error:  0.001663207508773779\n"
     ]
    }
   ],
   "source": [
    "model = evaluate_timeseries(std_df_train[:5000], 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part e: Determine Whether the Model Error is White Noise or Auto-correlated Using Ljung-Box Test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: White Noise. \n",
    "\n",
    "Justification: Running the Ljung-Box test on the HFT dataset, we obtain small test statistics, or p-values.\n",
    "As such, there does not exist sufficient evidence to reject the null hypothesis. Put another way, there\n",
    "does not exist reasonable evidence to conclude that the model error is autocorrelated, so we fail to\n",
    "reject the null hypothesis, concluding instead that the model error is white noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "T=100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_out_sample = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "residual = pred_out_sample-y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb,p=sm.stats.diagnostic.acorr_ljungbox(residual, lags=9, boxpierce=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.61658566,  5.49214988,  5.51013436,  6.04018039,  7.00702677,\n",
       "        9.10191919, 11.58875067, 17.51495799, 21.0254082 ])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.03166421, 0.06417928, 0.13803373, 0.19616761, 0.22011824,\n",
       "       0.16792682, 0.11492018, 0.02517188, 0.01253808])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Noah Grudowski MATH 527 Homework 3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
